use std::collections::{BTreeMap, BTreeSet, HashMap};
use std::fs;
use std::path::{Path, PathBuf};

use anyhow::{bail, Context, Result};
use chrono::Utc;
use clap::Args;
use serde::{Deserialize, Serialize};

const MODEL_BLOCK_BEGIN: &str = "// BEGIN AUTOGENERATED_LOCAL_REASONER_MODEL";
const MODEL_BLOCK_END: &str = "// END AUTOGENERATED_LOCAL_REASONER_MODEL";

#[derive(Debug, Clone, Args)]
pub struct TrainLocalReasonerArgs {
    #[arg(long, default_value = "kb/training/local_reasoner_training.jsonl")]
    pub dataset: PathBuf,

    #[arg(long, default_value_t = 512)]
    pub max_vocab: usize,

    #[arg(long, default_value_t = 1)]
    pub min_token_freq: usize,

    #[arg(
        long,
        default_value = "../ios-app/AtlasMasaIOS/Sources/Core/LocalReasoningEngine.swift"
    )]
    pub ios_output: PathBuf,

    #[arg(
        long,
        default_value = "../macos-app/AtlasMasaMacOS/Sources/Core/LocalReasoningEngine.swift"
    )]
    pub macos_output: PathBuf,

    #[arg(long, default_value = "../docs/ai/local-reasoner-model-report.md")]
    pub report_output: PathBuf,
}

#[derive(Debug, Clone, Deserialize)]
struct TrainingRow {
    prompt: String,
    label: String,
    #[serde(default)]
    next_action: Option<String>,
}

#[derive(Debug, Clone)]
struct EvalSummary {
    test_size: usize,
    accuracy: f64,
    per_label: BTreeMap<String, (usize, usize)>,
}

#[derive(Debug, Clone)]
struct TrainedWeights {
    log_priors: Vec<f64>,
    log_likelihoods: Vec<Vec<f64>>,
}

#[derive(Debug)]
struct ReportInput {
    dataset_path: PathBuf,
    sample_count: usize,
    synthetic_count: usize,
    labels: Vec<String>,
    vocab_size: usize,
    max_vocab: usize,
    min_token_freq: usize,
}

#[derive(Debug, Serialize)]
struct AppLocalReasoningModel {
    schema_version: u8,
    model_name: String,
    trained_at_utc: String,
    sample_count: usize,
    labels: Vec<String>,
    vocabulary: Vec<String>,
    log_priors: Vec<f64>,
    log_likelihoods: Vec<Vec<f64>>,
    next_actions: Vec<String>,
    label_briefs: Vec<String>,
    keyword_hints: Vec<Vec<String>>,
    keyword_bonus: f64,
}

pub fn train_and_write(args: TrainLocalReasonerArgs) -> Result<()> {
    if args.max_vocab == 0 {
        bail!("--max-vocab must be greater than 0");
    }

    let mut rows = load_dataset(&args.dataset)?;
    if rows.len() < 20 {
        bail!(
            "dataset too small ({} rows). provide at least 20 rows for stable local training",
            rows.len()
        );
    }

    let base_labels = collect_labels(&rows);
    let synthetic_rows = build_synthetic_rows(&base_labels);
    let synthetic_count = synthetic_rows.len();
    rows.extend(synthetic_rows);

    let labels = collect_labels(&rows);
    if labels.len() < 3 {
        bail!(
            "dataset currently contains {} labels. provide at least 3 labels",
            labels.len()
        );
    }

    let eval = evaluate_model(&rows, &labels, args.max_vocab, args.min_token_freq);

    let vocabulary = build_vocabulary(&rows, args.max_vocab, args.min_token_freq);
    if vocabulary.is_empty() {
        bail!("vocabulary is empty; reduce --min-token-freq or provide richer training text");
    }

    let trained = train_weights(&rows, &labels, &vocabulary);
    let next_actions = collect_next_actions(&rows, &labels);
    let label_briefs = labels
        .iter()
        .map(|label| label_brief(label))
        .collect::<Vec<_>>();
    let keyword_hints = collect_keyword_hints(&rows, &labels);

    let model = AppLocalReasoningModel {
        schema_version: 1,
        model_name: "atlas-local-reasoner-v3".to_string(),
        trained_at_utc: Utc::now().to_rfc3339(),
        sample_count: rows.len(),
        labels: labels.clone(),
        vocabulary: vocabulary.clone(),
        log_priors: trained.log_priors,
        log_likelihoods: trained.log_likelihoods,
        next_actions,
        label_briefs,
        keyword_hints,
        keyword_bonus: 1.6,
    };

    let model_json = serde_json::to_string(&model).context("serialize local model json")?;

    inject_model_block(&args.ios_output, &model_json)?;
    inject_model_block(&args.macos_output, &model_json)?;

    let report = build_report(
        ReportInput {
            dataset_path: args.dataset.clone(),
            sample_count: rows.len(),
            synthetic_count,
            labels,
            vocab_size: vocabulary.len(),
            max_vocab: args.max_vocab,
            min_token_freq: args.min_token_freq,
        },
        &eval,
    );

    if let Some(parent) = args.report_output.parent() {
        if !parent.as_os_str().is_empty() {
            fs::create_dir_all(parent)
                .with_context(|| format!("create report directory {}", parent.display()))?;
        }
    }

    fs::write(&args.report_output, report)
        .with_context(|| format!("write report {}", args.report_output.display()))?;

    println!(
        "trained local reasoner: samples={} labels={} vocab={} eval_acc={:.2}%",
        rows.len(),
        model.labels.len(),
        model.vocabulary.len(),
        eval.accuracy * 100.0
    );
    println!("updated {}", args.ios_output.display());
    println!("updated {}", args.macos_output.display());
    println!("wrote {}", args.report_output.display());

    Ok(())
}

fn load_dataset(path: &Path) -> Result<Vec<TrainingRow>> {
    let body = fs::read_to_string(path)
        .with_context(|| format!("read local training dataset {}", path.display()))?;

    let mut rows = Vec::new();
    for (line_idx, raw_line) in body.lines().enumerate() {
        let line = raw_line.trim();
        if line.is_empty() || line.starts_with('#') {
            continue;
        }

        let parsed: TrainingRow = serde_json::from_str(line).with_context(|| {
            format!(
                "parse dataset jsonl line {} in {}",
                line_idx + 1,
                path.display()
            )
        })?;

        if parsed.prompt.trim().is_empty() {
            bail!(
                "dataset line {} has empty prompt in {}",
                line_idx + 1,
                path.display()
            );
        }

        if parsed.label.trim().is_empty() {
            bail!(
                "dataset line {} has empty label in {}",
                line_idx + 1,
                path.display()
            );
        }

        rows.push(TrainingRow {
            prompt: parsed.prompt.trim().to_string(),
            label: normalize_label(parsed.label.as_str()),
            next_action: parsed
                .next_action
                .as_ref()
                .map(|value| value.trim().to_string())
                .filter(|value| !value.is_empty()),
        });
    }

    Ok(rows)
}

fn normalize_label(label: &str) -> String {
    label.trim().to_ascii_lowercase().replace([' ', '-'], "_")
}

fn build_synthetic_rows(labels: &[String]) -> Vec<TrainingRow> {
    let mut rows = Vec::new();

    for label in labels {
        for prompt in synthetic_prompts_for_label(label) {
            rows.push(TrainingRow {
                prompt: prompt.to_string(),
                label: label.clone(),
                next_action: Some(default_next_action(label)),
            });
        }
    }

    rows
}

fn synthetic_prompts_for_label(label: &str) -> Vec<&'static str> {
    match label {
        "execution_now" => vec![
            "Need immediate execution now with one focused action block.",
            "Start the first high-impact task today and lock momentum.",
            "I need direct action now, not more planning.",
            "Give me immediate execution sequence for this hour.",
            "Urgent today: prioritize and execute the first step now.",
        ],
        "revenue_focus" => vec![
            "Need revenue growth and cash flow action for today.",
            "Prioritize sales outreach and close one paying client.",
            "Money pressure is high, need income and pricing execution.",
            "What business move increases revenue the fastest right now?",
            "Focus on profit, clients, and direct monetization tasks.",
        ],
        "resilience_safety" => vec![
            "Emergency readiness: battery, signal, fallback, safe harbor.",
            "Safety protocol for continuity under risk and uncertainty.",
            "Need resilient backup for comms, navigation, and lock mode.",
            "What is the safest fallback path if systems fail tonight?",
            "Continuity planning for breakdown and remote emergency support.",
        ],
        "health_recovery" => vec![
            "Stress and burnout are high; need recovery-first execution.",
            "Cognitive fatigue is heavy, set rest and low-friction action.",
            "Health stabilization first, then one manageable work block.",
            "Need restorative routine to reduce overload and anxiety.",
            "Recovery mode with controlled output and gentle momentum.",
        ],
        "technical_debug" => vec![
            "API auth error and passkey flow failing in production.",
            "Debug deploy logs, isolate root cause, patch safely.",
            "OAuth callback broken, need reproducible fix and verification.",
            "Build failed with dependency conflict and runtime 502 error.",
            "Technical incident response: triage, patch, and retest now.",
        ],
        "strategy_long_horizon" => vec![
            "Long-horizon strategy for mission, wealth, and durable scale.",
            "Roadmap alignment across daily execution and decade goals.",
            "Define strategic milestone for capital growth and resilience.",
            "Need long-term planning for ecosystem expansion and stability.",
            "How do we protect mission trajectory over years?",
        ],
        "travel_ops" => vec![
            "Travel operations plan with legal overnight and service points.",
            "Route logistics for mobility workflow and regulated parking.",
            "Need itinerary with maintenance, refill, and safe hub fallback.",
            "Mobility ops for van route, legal sleep, and daily work blocks.",
            "Plan travel execution with practical stops and risk backups.",
        ],
        _ => Vec::new(),
    }
}

fn collect_labels(rows: &[TrainingRow]) -> Vec<String> {
    let mut set = BTreeSet::new();
    for row in rows {
        set.insert(row.label.clone());
    }
    set.into_iter().collect()
}

fn tokenize(input: &str) -> Vec<String> {
    let mut unigrams = Vec::new();
    let mut current = String::new();

    for ch in input.chars() {
        if ch.is_alphanumeric() {
            for lowered in ch.to_lowercase() {
                current.push(lowered);
            }
        } else if !current.is_empty() {
            unigrams.push(std::mem::take(&mut current));
        }
    }

    if !current.is_empty() {
        unigrams.push(current);
    }

    let unigrams = unigrams
        .into_iter()
        .filter(|token| token.chars().count() >= 2)
        .collect::<Vec<_>>();

    let mut tokens = unigrams.clone();
    for pair in unigrams.windows(2) {
        if let [left, right] = pair {
            tokens.push(format!("{left}_{right}"));
        }
    }

    tokens
}

fn build_vocabulary(rows: &[TrainingRow], max_vocab: usize, min_token_freq: usize) -> Vec<String> {
    let mut freq = HashMap::<String, usize>::new();
    for row in rows {
        for token in tokenize(&row.prompt) {
            *freq.entry(token).or_insert(0) += 1;
        }
    }

    let mut scored = freq
        .into_iter()
        .filter(|(_, count)| *count >= min_token_freq)
        .collect::<Vec<_>>();

    scored.sort_by(|a, b| b.1.cmp(&a.1).then_with(|| a.0.cmp(&b.0)));

    scored
        .into_iter()
        .take(max_vocab)
        .map(|(token, _)| token)
        .collect()
}

fn train_weights(rows: &[TrainingRow], labels: &[String], vocabulary: &[String]) -> TrainedWeights {
    let label_index = labels
        .iter()
        .enumerate()
        .map(|(idx, label)| (label.clone(), idx))
        .collect::<HashMap<_, _>>();

    let vocab_index = vocabulary
        .iter()
        .enumerate()
        .map(|(idx, token)| (token.as_str(), idx))
        .collect::<HashMap<_, _>>();

    let label_count = labels.len();
    let vocab_size = vocabulary.len();

    let mut doc_counts = vec![0usize; label_count];
    let mut token_counts = vec![vec![0usize; vocab_size]; label_count];
    let mut label_token_totals = vec![0usize; label_count];

    for row in rows {
        let Some(&label_idx) = label_index.get(&row.label) else {
            continue;
        };

        doc_counts[label_idx] += 1;

        for token in tokenize(&row.prompt) {
            if let Some(&tok_idx) = vocab_index.get(token.as_str()) {
                token_counts[label_idx][tok_idx] += 1;
                label_token_totals[label_idx] += 1;
            }
        }
    }

    let total_docs = rows.len() as f64;
    let prior_denom = total_docs + label_count as f64;

    let log_priors = doc_counts
        .iter()
        .map(|count| ((*count as f64 + 1.0) / prior_denom).ln())
        .collect::<Vec<_>>();

    let mut log_likelihoods = vec![vec![0.0_f64; vocab_size]; label_count];

    for label_idx in 0..label_count {
        let denom = label_token_totals[label_idx] as f64 + vocab_size as f64;
        for tok_idx in 0..vocab_size {
            let count = token_counts[label_idx][tok_idx] as f64;
            log_likelihoods[label_idx][tok_idx] = ((count + 1.0) / denom).ln();
        }
    }

    TrainedWeights {
        log_priors,
        log_likelihoods,
    }
}

fn predict_label(
    prompt: &str,
    labels: &[String],
    vocabulary: &[String],
    weights: &TrainedWeights,
    keyword_hints: &[Vec<String>],
    keyword_bonus: f64,
) -> usize {
    let vocab_index = vocabulary
        .iter()
        .enumerate()
        .map(|(idx, token)| (token.as_str(), idx))
        .collect::<HashMap<_, _>>();

    let mut token_hits = HashMap::<usize, usize>::new();
    let tokens = tokenize(prompt);
    for token in &tokens {
        if let Some(&tok_idx) = vocab_index.get(token.as_str()) {
            *token_hits.entry(tok_idx).or_insert(0) += 1;
        }
    }
    let token_set = tokens.into_iter().collect::<BTreeSet<_>>();

    let mut best_idx = 0usize;
    let mut best_score = f64::NEG_INFINITY;

    for label_idx in 0..labels.len() {
        let mut score = weights.log_priors[label_idx];
        for (tok_idx, count) in &token_hits {
            score += weights.log_likelihoods[label_idx][*tok_idx] * *count as f64;
        }
        let hint_matches = keyword_hints
            .get(label_idx)
            .map(|hints| {
                hints
                    .iter()
                    .filter(|token| token_set.contains(*token))
                    .count()
            })
            .unwrap_or(0);
        score += hint_matches as f64 * keyword_bonus;

        if score > best_score {
            best_score = score;
            best_idx = label_idx;
        }
    }

    best_idx
}

fn evaluate_model(
    rows: &[TrainingRow],
    labels: &[String],
    max_vocab: usize,
    min_token_freq: usize,
) -> EvalSummary {
    if rows.len() < 8 {
        return EvalSummary {
            test_size: 0,
            accuracy: 1.0,
            per_label: BTreeMap::new(),
        };
    }

    let mut train = Vec::new();
    let mut test = Vec::new();

    for (idx, row) in rows.iter().enumerate() {
        if idx % 5 == 0 {
            test.push(row.clone());
        } else {
            train.push(row.clone());
        }
    }

    if train.is_empty() || test.is_empty() {
        return EvalSummary {
            test_size: 0,
            accuracy: 1.0,
            per_label: BTreeMap::new(),
        };
    }

    let vocabulary = build_vocabulary(&train, max_vocab, min_token_freq);
    if vocabulary.is_empty() {
        return EvalSummary {
            test_size: test.len(),
            accuracy: 0.0,
            per_label: BTreeMap::new(),
        };
    }

    let trained = train_weights(&train, labels, &vocabulary);
    let keyword_hints = collect_keyword_hints(&train, labels);

    let mut correct = 0usize;
    let mut per_label = BTreeMap::<String, (usize, usize)>::new();

    for row in &test {
        let predicted_idx = predict_label(
            &row.prompt,
            labels,
            &vocabulary,
            &trained,
            &keyword_hints,
            1.6,
        );
        let predicted = labels[predicted_idx].clone();
        let matches = predicted == row.label;

        let entry = per_label.entry(row.label.clone()).or_insert((0, 0));
        entry.1 += 1;
        if matches {
            correct += 1;
            entry.0 += 1;
        }
    }

    EvalSummary {
        test_size: test.len(),
        accuracy: correct as f64 / test.len() as f64,
        per_label,
    }
}

fn collect_next_actions(rows: &[TrainingRow], labels: &[String]) -> Vec<String> {
    labels
        .iter()
        .map(|label| {
            let mut counts = HashMap::<String, usize>::new();
            for row in rows {
                if &row.label == label {
                    if let Some(action) = row.next_action.as_ref() {
                        *counts.entry(action.clone()).or_insert(0) += 1;
                    }
                }
            }

            counts
                .into_iter()
                .max_by(|a, b| a.1.cmp(&b.1).then_with(|| a.0.cmp(&b.0)))
                .map(|(value, _)| value)
                .unwrap_or_else(|| default_next_action(label))
        })
        .collect()
}

fn collect_keyword_hints(rows: &[TrainingRow], labels: &[String]) -> Vec<Vec<String>> {
    let mut global_counts = HashMap::<String, usize>::new();
    let mut per_label_counts = HashMap::<String, HashMap<String, usize>>::new();

    for row in rows {
        let unique_tokens = tokenize(&row.prompt).into_iter().collect::<BTreeSet<_>>();
        let label_counts = per_label_counts.entry(row.label.clone()).or_default();
        for token in unique_tokens {
            *global_counts.entry(token.clone()).or_insert(0) += 1;
            *label_counts.entry(token).or_insert(0) += 1;
        }
    }

    labels
        .iter()
        .map(|label| {
            let Some(label_counts) = per_label_counts.get(label) else {
                return seed_keywords_for_label(label);
            };

            let mut scored = label_counts
                .iter()
                .map(|(token, label_freq)| {
                    let global = *global_counts.get(token).unwrap_or(&1) as f64;
                    let score = (*label_freq as f64 + 0.5) / (global + 1.0);
                    (token.clone(), score, *label_freq)
                })
                .collect::<Vec<_>>();

            scored.sort_by(|a, b| {
                b.1.partial_cmp(&a.1)
                    .unwrap_or(std::cmp::Ordering::Equal)
                    .then_with(|| b.2.cmp(&a.2))
                    .then_with(|| a.0.cmp(&b.0))
            });

            let mut combined = seed_keywords_for_label(label);
            for (token, _, _) in scored.into_iter().take(16) {
                combined.push(token);
            }

            let mut seen = BTreeSet::new();
            combined
                .into_iter()
                .filter(|token| seen.insert(token.clone()))
                .take(20)
                .collect::<Vec<_>>()
        })
        .collect()
}

fn seed_keywords_for_label(label: &str) -> Vec<String> {
    let raw: &[&str] = match label {
        "execution_now" => &[
            "now",
            "today",
            "immediate",
            "start",
            "execute",
            "action",
            "momentum",
        ],
        "revenue_focus" => &[
            "revenue", "money", "cash", "client", "sales", "income", "pricing", "profit",
        ],
        "resilience_safety" => &[
            "emergency",
            "safe",
            "safety",
            "fallback",
            "signal",
            "battery",
            "risk",
            "continuity",
            "resilience",
        ],
        "health_recovery" => &[
            "health",
            "stress",
            "burnout",
            "recovery",
            "fatigue",
            "cognitive",
            "rest",
            "stabilize",
        ],
        "technical_debug" => &[
            "error", "bug", "deploy", "oauth", "api", "passkey", "build", "debug", "logs", "502",
        ],
        "strategy_long_horizon" => &[
            "strategy", "long", "mission", "roadmap", "wealth", "decade", "scale", "horizon",
            "capital",
        ],
        "travel_ops" => &[
            "route",
            "travel",
            "camp",
            "overnight",
            "legal",
            "parking",
            "itinerary",
            "mobility",
            "van",
            "logistics",
        ],
        _ => &[],
    };

    raw.iter().map(|value| (*value).to_string()).collect()
}

fn label_brief(label: &str) -> String {
    match label {
        "execution_now" => "Immediate execution protocol".to_string(),
        "revenue_focus" => "Revenue acceleration channel".to_string(),
        "resilience_safety" => "Continuity and risk management".to_string(),
        "health_recovery" => "Cognitive and health stabilization".to_string(),
        "technical_debug" => "Systems debugging and delivery".to_string(),
        "strategy_long_horizon" => "Long-horizon strategy and capital".to_string(),
        "travel_ops" => "Mobility operations and route logistics".to_string(),
        other => other.replace('_', " "),
    }
}

fn default_next_action(label: &str) -> String {
    match label {
        "execution_now" => {
            "Execute one high-impact step in the next 15 minutes, then lock the next checkpoint."
                .to_string()
        }
        "revenue_focus" => {
            "Prioritize one direct revenue move now and schedule a same-day follow-up."
                .to_string()
        }
        "resilience_safety" => {
            "Run continuity checks: power, comms, offline navigation, and safe-harbor path."
                .to_string()
        }
        "health_recovery" => {
            "Reduce cognitive load, stabilize recovery conditions, and execute one low-friction action."
                .to_string()
        }
        "technical_debug" => {
            "Capture the failing signal, isolate one root cause, and apply a minimal verified fix."
                .to_string()
        }
        "strategy_long_horizon" => {
            "Define the next strategic milestone and commit one measurable action this week."
                .to_string()
        }
        "travel_ops" => {
            "Confirm route legality, service points, and overnight fallback before departure."
                .to_string()
        }
        _ => "Define daily, mid-term, and long-term actions, then execute step one now.".to_string(),
    }
}

fn inject_model_block(swift_path: &Path, model_json: &str) -> Result<()> {
    let content = fs::read_to_string(swift_path)
        .with_context(|| format!("read swift file {}", swift_path.display()))?;

    let start = content
        .find(MODEL_BLOCK_BEGIN)
        .with_context(|| format!("missing begin marker in {}", swift_path.display()))?;

    let end = content
        .find(MODEL_BLOCK_END)
        .with_context(|| format!("missing end marker in {}", swift_path.display()))?;

    if start >= end {
        bail!("invalid marker ordering in {}", swift_path.display());
    }

    let before = &content[..start];
    let after = &content[end + MODEL_BLOCK_END.len()..];

    let block = format!(
        "{begin}\nprivate let defaultLocalReasoningModelJSON = #\"\"\"\n{json}\n\"\"\"#\n{end}",
        begin = MODEL_BLOCK_BEGIN,
        json = model_json,
        end = MODEL_BLOCK_END,
    );

    let updated = format!("{}{}{}", before, block, after);

    fs::write(swift_path, updated)
        .with_context(|| format!("write swift file {}", swift_path.display()))?;

    Ok(())
}

fn build_report(input: ReportInput, eval: &EvalSummary) -> String {
    let mut report = String::new();
    report.push_str("# Local Reasoner Training Report\n\n");
    report.push_str(&format!(
        "- Generated at (UTC): {}\n",
        Utc::now().to_rfc3339()
    ));
    report.push_str(&format!("- Dataset: `{}`\n", input.dataset_path.display()));
    report.push_str(&format!("- Samples: {}\n", input.sample_count));
    report.push_str(&format!(
        "- Synthetic augmentation samples: {}\n",
        input.synthetic_count
    ));
    report.push_str(&format!("- Labels: {}\n", input.labels.join(", ")));
    report.push_str(&format!("- Vocabulary size: {}\n", input.vocab_size));
    report.push_str(&format!("- Max vocab configured: {}\n", input.max_vocab));
    report.push_str(&format!(
        "- Min token frequency configured: {}\n",
        input.min_token_freq
    ));
    report.push_str(&format!(
        "- Holdout accuracy: {:.2}% (test size: {})\n\n",
        eval.accuracy * 100.0,
        eval.test_size
    ));

    report.push_str("## Holdout Label Breakdown\n\n");
    if eval.per_label.is_empty() {
        report.push_str("No holdout split available for this dataset size.\n");
    } else {
        report.push_str("| Label | Correct | Total | Accuracy |\n");
        report.push_str("| --- | ---: | ---: | ---: |\n");
        for (label, (correct, total)) in &eval.per_label {
            let accuracy = if *total == 0 {
                0.0
            } else {
                (*correct as f64 / *total as f64) * 100.0
            };
            report.push_str(&format!(
                "| {} | {} | {} | {:.2}% |\n",
                label, correct, total, accuracy
            ));
        }
    }

    report.push_str("\n## Notes\n\n");
    report
        .push_str("- This model is optimized for low-latency on-device inference in Swift apps.\n");
    report.push_str(
        "- Expand dataset coverage continuously; retrain after material product/policy updates.\n",
    );
    report
        .push_str("- Use structured labels to keep execution suggestions predictable and safe.\n");

    report
}
